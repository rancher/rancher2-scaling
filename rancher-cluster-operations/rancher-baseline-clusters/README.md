This terraform is for creating the downstream RKE clusters to test Ranchers scaling ability. It contains two root modules, one for AWS clusters and one for Linode clusters.

### How to apply the Linode module:
1. Navigate into the desired cloud provider folder `cd linode`
2. Run `terraform init` to get providers/plugins downloaded
3. Create a `terraform.tfvars` file.

   This is an example `terraform.tfvars` for the linode module. See `variables.tf` for more options.
   ```
    rancher_api_url        = "<rancher server url>"
    rancher_token_key      = "<rancher token>"
    linode_token           = "<linode token>"
    server_instance_type   = "g6-dedicated-4"
    region                 = "us-west"
    image                  = "linode/ubuntu18.04"
    k8s_version            = "v1.23.6-rancher1-1"
    install_docker_version = "20.10"
    ```

    Notes:
    * Variables can be set in the `terraform.tfvars` file, in the env, or on the command line when calling `terraform apply`.
    * The total number of nodes per cluster will be `var.nodes_per_pool` * `var.node_pool_count`.
    * Each root module has it's own README.md generated by `terraform-docs`.

### To create a batch of clusters:
  1. Navigate into the desired cloud provider folder `cd linode`
  2. Call `../provision_clusters.sh N` where `N` is the number of clusters required
### To perform a simple scale-in process:
  1. Navigate into the desired cloud provider folder `cd linode`
  2. Call `../batch_scale.sh <absolute path to control-plane's kubeconfig> <number of clusters per batch> <target number of clusters>`
        * Example: `../batch_scale.sh /Users/my_user/path/to/control-plane-kubeconfig 50 150`
  3. After each batch of clusters the script will wait 9 minutes to give the Rancher server time to finish provisioning the clusters
  4. At the halfway point and at the end of the run the script will collect heap logs and store them in the cloud provider

### To cleanup:
  1. Navigate into cloud provider folder where clusters were created `cd linode`
  2. Call `../destroy_clusters.sh`

  Notes:
  * This script will perform a `terraform destroy` operation in reverse-order on each workspace created by the provisioning script. However this will also delete all the clusters from Rancher and with the cluster GC being slow this can take a long time (hours) on a large setup. The faster way, assuming the control-plane is coming down as well, is to just delete the hosts from the cloud provider and tear down the control-plane.

  * Alternatively, manually deleting the clusters via the UI is somewhat faster. You may need to verify the nodes were deleted properly as it is possible for nodes to become de-synced and replaced by Rancher without actually being terminated in the cloud provider.
